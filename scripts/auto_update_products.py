#!/usr/bin/env python3
"""
UchiGift å•†å“ãƒ‡ãƒ¼ã‚¿è‡ªå‹•æ›´æ–°ã‚·ã‚¹ãƒ†ãƒ 

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ©Ÿèƒ½:
1. æ¥½å¤©APIå•†å“å–å¾— (fetch_rakuten_products.py)
2. ã‚¸ãƒ£ãƒ³ãƒ«åˆ†æãƒ»è‡ªå‹•åˆ†é¡ (analyze_genres.py)
3. ãƒ¦ãƒ¼ã‚¶ãƒ¼ç¢ºèªãƒ»å€‹åˆ¥èª¿æ•´ (ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–)
4. Meilisearchå†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (index_meili_products.py)

ä½¿ç”¨æ–¹æ³•:
python auto_update_products.py [--no-fetch] [--no-interactive] [--max-items 5000]
"""

import os
import sys
import json
import time
import subprocess
import argparse
from pathlib import Path
from typing import Dict, List, Set, Optional
from datetime import datetime

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’æœ€åˆã«èª­ã¿è¾¼ã¿
from dotenv import load_dotenv
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
load_dotenv(Path(__file__).parent.parent / '.env')


class HAREGiftAutoUpdater:
    def __init__(self, no_fetch: bool = False, no_interactive: bool = False, max_items: int = 5000, 
                 keep_backups: int = 5, cleanup_old: bool = True):
        self.no_fetch = no_fetch
        self.no_interactive = no_interactive
        self.max_items = max_items
        self.keep_backups = keep_backups  # ä¿æŒã™ã‚‹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ•°
        self.cleanup_old = cleanup_old    # å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«è‡ªå‹•å‰Šé™¤
        self.data_dir = Path('data')
        self.scripts_dir = Path('.')
        
        # ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡ŒçŠ¶æ³
        self.steps = {
            'cleanup': False,
            'fetch': False,
            'analyze': False,
            'auto_map': False,
            'interactive': False,
            'reindex': False
        }

    def log(self, message: str, level: str = "INFO"):
        """ãƒ­ã‚°å‡ºåŠ›"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print(f"[{timestamp}] {level}: {message}")

    def run_command(self, command: List[str], description: str) -> bool:
        """å¤–éƒ¨ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ"""
        self.log(f"å®Ÿè¡Œä¸­: {description}")
        try:
            result = subprocess.run(command, capture_output=True, text=True)
            if result.returncode != 0:
                self.log(f"ã‚¨ãƒ©ãƒ¼: {description} å¤±æ•—", "ERROR")
                self.log(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {result.stderr}", "ERROR")
                return False
            self.log(f"å®Œäº†: {description}")
            return True
        except Exception as e:
            self.log(f"ä¾‹å¤–ã‚¨ãƒ©ãƒ¼: {description} - {e}", "ERROR")
            return False

    def step_0_cleanup_old_files(self):
        """ã‚¹ãƒ†ãƒƒãƒ—0: å¤ã„ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if not self.cleanup_old:
            self.log("ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            self.steps['cleanup'] = True
            return
        
        self.log("ã‚¹ãƒ†ãƒƒãƒ—0: å¤ã„ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–‹å§‹")
        
        # manage_data_files.pyã‚’ä½¿ç”¨ã—ã¦ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
        command = [
            'python', 'manage_data_files.py', 
            '--cleanup', 
            '--keep', str(self.keep_backups),
            '--auto-confirm'  # è‡ªå‹•å®Ÿè¡Œã®ãŸã‚ç¢ºèªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—
        ]
        
        if self.run_command(command, f"å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆ{self.keep_backups}å€‹ä¿æŒï¼‰"):
            self.steps['cleanup'] = True
            self.log(f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: {self.keep_backups}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿æŒ")
        else:
            self.log("ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã«å¤±æ•—ã—ã¾ã—ãŸ", "ERROR")
            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¤±æ•—ã§ã‚‚å‡¦ç†ã‚’ç¶™ç¶š
            self.steps['cleanup'] = True

    def step_1_fetch_products(self) -> Optional[str]:
        """ã‚¹ãƒ†ãƒƒãƒ—1: æ¥½å¤©å•†å“ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        if self.no_fetch:
            self.log("æ¥½å¤©APIå–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆæ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ã«å¯¾å¿œï¼‰
            rakuten_dir = self.data_dir / 'sources' / 'rakuten'
            existing_files = list(rakuten_dir.glob('rakuten_haregift_products_*.json'))
            if not existing_files:
                # æ—§ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚æ¤œç´¢
                existing_files = list(rakuten_dir.glob('rakuten_uchiwai_products_*.json'))
                if not existing_files:
                    # ãƒ«ãƒ¼ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã§ã‚‚æ¤œç´¢ï¼ˆäº’æ›æ€§ï¼‰
                    existing_files = list(self.data_dir.glob('rakuten_*_products_*.json'))
            
            if existing_files:
                latest_file = max(existing_files, key=lambda x: x.stat().st_mtime)
                self.log(f"æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {latest_file.name}")
                self.steps['fetch'] = True
                return str(latest_file)
            else:
                self.log("æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", "ERROR")
                return None

        self.log("ã‚¹ãƒ†ãƒƒãƒ—1: æ¥½å¤©å•†å“ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹")
        
        # fetch_rakuten_products.pyã‚’å®Ÿè¡Œ
        if not self.run_command(['python', 'fetch_rakuten_products.py'], 'æ¥½å¤©å•†å“ãƒ‡ãƒ¼ã‚¿å–å¾—'):
            return None
        
        # æ–°ã—ãä½œæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆæ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ï¼‰
        rakuten_dir = self.data_dir / 'sources' / 'rakuten'
        latest_files = list(rakuten_dir.glob('rakuten_haregift_products_*.json'))
        if not latest_files:
            # æ—§ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚æ¤œç´¢
            latest_files = list(rakuten_dir.glob('rakuten_uchiwai_products_*.json'))
            if not latest_files:
                # ãƒ«ãƒ¼ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã§ã‚‚æ¤œç´¢ï¼ˆäº’æ›æ€§ï¼‰
                latest_files = list(self.data_dir.glob('rakuten_*_products_*.json'))
        
        if not latest_files:
            self.log("æ¥½å¤©ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ", "ERROR")
            return None
        
        latest_file = max(latest_files, key=lambda x: x.stat().st_mtime)
        self.log(f"ã‚¹ãƒ†ãƒƒãƒ—1å®Œäº†: {latest_file.name}")
        self.steps['fetch'] = True
        return str(latest_file)

    def step_2_analyze_genres(self) -> Dict:
        """ã‚¹ãƒ†ãƒƒãƒ—2: ã‚¸ãƒ£ãƒ³ãƒ«åˆ†æ"""
        self.log("ã‚¹ãƒ†ãƒƒãƒ—2: ã‚¸ãƒ£ãƒ³ãƒ«åˆ†æé–‹å§‹")
        
        # analyze_genres.pyã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç›´æ¥å®Ÿè¡Œ
        from analyze_genres import load_genre_cache, load_mapping_rules, suggest_category
        
        genre_cache = load_genre_cache()
        rules = load_mapping_rules()
        exact_mappings = rules.get('exact_mappings', {})
        
        if not genre_cache:
            self.log("genre_cache.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", "ERROR")
            return {}
        
        # æœªãƒãƒƒãƒ”ãƒ³ã‚°ã‚¸ãƒ£ãƒ³ãƒ«ã‚’æ¤œå‡º
        unmapped_genres = []
        for genre_id, genre_name in genre_cache.items():
            if genre_name and genre_name not in exact_mappings:
                suggestion = suggest_category(genre_name, rules)
                unmapped_genres.append({
                    'id': genre_id,
                    'name': genre_name,
                    'suggestion': suggestion
                })
        
        # åˆ†é¡å€™è£œåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        by_suggestion = {}
        for genre in unmapped_genres:
            suggestion = genre['suggestion']
            if suggestion not in by_suggestion:
                by_suggestion[suggestion] = []
            by_suggestion[suggestion].append(genre)
        
        self.log(f"ç·ã‚¸ãƒ£ãƒ³ãƒ«æ•°: {len(genre_cache)}")
        self.log(f"ãƒãƒƒãƒ”ãƒ³ã‚°æ¸ˆã¿: {len(exact_mappings)}")
        self.log(f"æœªãƒãƒƒãƒ”ãƒ³ã‚°: {len(unmapped_genres)}")
        
        for suggestion, genres in by_suggestion.items():
            self.log(f"  {suggestion}: {len(genres)}ä»¶")
        
        self.log("ã‚¹ãƒ†ãƒƒãƒ—2å®Œäº†: ã‚¸ãƒ£ãƒ³ãƒ«åˆ†æ")
        self.steps['analyze'] = True
        return by_suggestion

    def step_3_auto_mapping(self, analysis_result: Dict) -> Dict[str, str]:
        """ã‚¹ãƒ†ãƒƒãƒ—3: è‡ªå‹•ãƒãƒƒãƒ”ãƒ³ã‚°ææ¡ˆãƒ»é©ç”¨"""
        self.log("ã‚¹ãƒ†ãƒƒãƒ—3: è‡ªå‹•ãƒãƒƒãƒ”ãƒ³ã‚°é©ç”¨é–‹å§‹")
        
        auto_mappings = {}
        
        # ä¿¡é ¼åº¦ã®é«˜ã„ã‚«ãƒ†ã‚´ãƒªã‚’è‡ªå‹•é©ç”¨
        trusted_categories = ['food', 'drink', 'home', 'catalog', 'craft']
        
        for category in trusted_categories:
            if category in analysis_result:
                genres = analysis_result[category]
                self.log(f"{category}ã‚«ãƒ†ã‚´ãƒª: {len(genres)}ä»¶ã‚’è‡ªå‹•ãƒãƒƒãƒ”ãƒ³ã‚°")
                
                for genre in genres[:20]:  # ä¸Šä½20ä»¶ã¾ã§è‡ªå‹•é©ç”¨
                    auto_mappings[genre['name']] = category
                    self.log(f"  è¿½åŠ : {genre['name']} â†’ {category}")
        
        # ãƒ«ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
        if auto_mappings:
            self.update_mapping_rules(auto_mappings)
            self.log(f"âœ… ã‚¹ãƒ†ãƒƒãƒ—3å®Œäº†: {len(auto_mappings)}ä»¶ã‚’è‡ªå‹•ãƒãƒƒãƒ”ãƒ³ã‚°")
        else:
            self.log("âœ… ã‚¹ãƒ†ãƒƒãƒ—3å®Œäº†: æ–°ã—ã„è‡ªå‹•ãƒãƒƒãƒ”ãƒ³ã‚°ãªã—")
        
        self.steps['auto_map'] = True
        return auto_mappings

    def step_4_interactive_mapping(self, analysis_result: Dict) -> Dict[str, str]:
        """ã‚¹ãƒ†ãƒƒãƒ—4: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"""
        if self.no_interactive:
            self.log("ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            self.steps['interactive'] = True
            return {}

        self.log("ğŸ¨ ã‚¹ãƒ†ãƒƒãƒ—4: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒƒãƒ”ãƒ³ã‚°é–‹å§‹")
        
        # æœªå‡¦ç†ã®ã‚¸ãƒ£ãƒ³ãƒ«ï¼ˆunknownã‚«ãƒ†ã‚´ãƒªï¼‰ã®ã¿ã‚’å¯¾è±¡
        unknown_genres = analysis_result.get('unknown', [])
        if not unknown_genres:
            self.log("ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å‡¦ç†ãŒå¿…è¦ãªã‚¸ãƒ£ãƒ³ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            self.steps['interactive'] = True
            return {}
        
        self.log(f"æœªåˆ†é¡ã‚¸ãƒ£ãƒ³ãƒ« {len(unknown_genres)}ä»¶ã®ç¢ºèªã‚’é–‹å§‹ã—ã¾ã™")
        self.log("(Enterã§ã‚¹ã‚­ãƒƒãƒ—ã€q ã§çµ‚äº†)")
        
        new_mappings = {}
        categories = ['food', 'drink', 'home', 'catalog', 'craft', 'exclude']
        
        for i, genre in enumerate(unknown_genres[:10], 1):  # æœ€åˆã®10ä»¶ã®ã¿
            print(f"\n[{i}/{min(10, len(unknown_genres))}] ã‚¸ãƒ£ãƒ³ãƒ«: '{genre['name']}'")
            print(f"é¸æŠè‚¢: {', '.join(categories)}")
            
            choice = input(f"åˆ†é¡ã‚’é¸æŠ (Enterã§ã‚¹ã‚­ãƒƒãƒ—): ").strip().lower()
            
            if choice == 'q':
                break
            elif choice == '':
                continue
            elif choice in categories:
                if choice != 'exclude':
                    new_mappings[genre['name']] = choice
                    print(f"âœ… '{genre['name']}' â†’ {choice}")
                else:
                    print(f"â¡ï¸ '{genre['name']}' ã‚’é™¤å¤–")
            else:
                print(f"ç„¡åŠ¹ãªé¸æŠ: {choice}")
        
        if new_mappings:
            self.update_mapping_rules(new_mappings)
            self.log(f"âœ… ã‚¹ãƒ†ãƒƒãƒ—4å®Œäº†: {len(new_mappings)}ä»¶ã‚’ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒƒãƒ”ãƒ³ã‚°")
        else:
            self.log("âœ… ã‚¹ãƒ†ãƒƒãƒ—4å®Œäº†: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒƒãƒ”ãƒ³ã‚°ãªã—")
        
        self.steps['interactive'] = True
        return new_mappings

    def step_5_reindex(self, data_file: str) -> bool:
        """ã‚¹ãƒ†ãƒƒãƒ—5: Meilisearchå†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ï¼‹æœ¬ç•ªç’°å¢ƒï¼‰"""
        self.log("ğŸ“š ã‚¹ãƒ†ãƒƒãƒ—5: Meilisearchå†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é–‹å§‹")
        
        # MeiliSearchè¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
        meili_url = os.getenv('MEILI_URL')
        meili_key = os.getenv('MEILI_KEY')
        
        if not meili_url or not meili_key:
            raise ValueError("MEILI_URL ã¾ãŸã¯ MEILI_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        self.log(f"MeiliSearch: {meili_url}")
        
        # MeiliSearchå‡¦ç†
        self.log("MeiliSearch: æ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å‰Šé™¤ä¸­...")
        delete_cmd = [
            'python', '-c',
            f"import requests; requests.delete('{meili_url}/indexes/items', headers={{'Authorization': 'Bearer {meili_key}'}}); print('ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‰Šé™¤å®Œäº†')"
        ]
        
        if not self.run_command(delete_cmd, "æ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‰Šé™¤"):
            self.log("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸãŒç¶šè¡Œã—ã¾ã™", "WARNING")
        
        time.sleep(2)  # Meilisearchå‡¦ç†å¾…æ©Ÿ
        
        # MeiliSearchå†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®Ÿè¡Œ
        self.log("MeiliSearch: ãƒ‡ãƒ¼ã‚¿æŠ•å…¥é–‹å§‹...")
        reindex_cmd = [
            'python', 'index_meili_products.py',
            '--source', 'rakuten',
            '--file', data_file
        ]
        
        if not self.run_command(reindex_cmd, "Meilisearchå†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"):
            self.log("å†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤±æ•—", "ERROR")
            return False
        
        time.sleep(5)  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®Œäº†å¾…æ©Ÿ
        
        # çµæœç¢ºèª
        self.log("åˆ†é¡çµæœã‚’ç¢ºèªä¸­...")
        check_cmd = [
            'python', '-c',
            f"import requests; resp = requests.get('{meili_url}/indexes/items/search', params={{'facets': ['genre_group']}}, headers={{'Authorization': 'Bearer {meili_key}'}}); facets = resp.json().get('facetDistribution', {{}}); print('æœ€çµ‚ã‚¸ãƒ£ãƒ³ãƒ«åˆ†å¸ƒ:'); [print(f'  {{k}}: {{v}}å€‹') for k, v in facets.get('genre_group', {{}}).items()]"
        ]
        
        check_success = self.run_command(check_cmd, "åˆ†é¡çµæœç¢ºèª")
        
        if check_success:
            self.log("âœ… ã‚¹ãƒ†ãƒƒãƒ—5å®Œäº†: å†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æˆåŠŸ")
        
        if check_success:
            self.log("âœ… ã‚¹ãƒ†ãƒƒãƒ—5å®Œäº†: å†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æˆåŠŸ")
            self.steps['reindex'] = True
            return True
        
        return False

    def update_mapping_rules(self, new_mappings: Dict[str, str]):
        """ã‚¸ãƒ£ãƒ³ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ«ãƒ¼ãƒ«ã‚’æ›´æ–°"""
        rules_file = self.data_dir / 'genre_mapping_rules.json'
        
        try:
            with open(rules_file, 'r', encoding='utf-8') as f:
                rules = json.load(f)
            
            if 'exact_mappings' not in rules:
                rules['exact_mappings'] = {}
            
            rules['exact_mappings'].update(new_mappings)
            
            with open(rules_file, 'w', encoding='utf-8') as f:
                json.dump(rules, f, ensure_ascii=False, indent=2)
            
            self.log(f"ãƒ«ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°: {len(new_mappings)}ä»¶è¿½åŠ ")
            
        except Exception as e:
            self.log(f"ãƒ«ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}", "ERROR")

    def run_full_pipeline(self):
        """å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        self.log("HAREGift å•†å“ãƒ‡ãƒ¼ã‚¿è‡ªå‹•æ›´æ–°ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
        self.log("=" * 60)
        
        start_time = time.time()
        
        try:
            # ã‚¹ãƒ†ãƒƒãƒ—0: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            self.step_0_cleanup_old_files()
            
            # ã‚¹ãƒ†ãƒƒãƒ—1: å•†å“ãƒ‡ãƒ¼ã‚¿å–å¾—
            data_file = self.step_1_fetch_products()
            if not data_file:
                raise Exception("å•†å“ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—")
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: ã‚¸ãƒ£ãƒ³ãƒ«åˆ†æ
            analysis_result = self.step_2_analyze_genres()
            if not analysis_result:
                raise Exception("ã‚¸ãƒ£ãƒ³ãƒ«åˆ†æã«å¤±æ•—")
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: è‡ªå‹•ãƒãƒƒãƒ”ãƒ³ã‚°
            auto_mappings = self.step_3_auto_mapping(analysis_result)
            
            # ã‚¹ãƒ†ãƒƒãƒ—4: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒƒãƒ”ãƒ³ã‚°
            interactive_mappings = self.step_4_interactive_mapping(analysis_result)
            
            # ã‚¹ãƒ†ãƒƒãƒ—5: å†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            if not self.step_5_reindex(data_file):
                raise Exception("å†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤±æ•—")
            
            # ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡ã®ç¢ºèª
            self.report_disk_usage()
            
            # å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ
            elapsed_time = time.time() - start_time
            self.log("=" * 60)
            self.log("è‡ªå‹•æ›´æ–°ã‚·ã‚¹ãƒ†ãƒ å®Œäº†!")
            self.log(f"å®Ÿè¡Œæ™‚é–“: {elapsed_time:.1f}ç§’")
            self.log(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {data_file}")
            self.log(f"è‡ªå‹•ãƒãƒƒãƒ”ãƒ³ã‚°: {len(auto_mappings)}ä»¶")
            self.log(f"æ‰‹å‹•ãƒãƒƒãƒ”ãƒ³ã‚°: {len(interactive_mappings)}ä»¶")
            
            # ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡ŒçŠ¶æ³
            self.log("\nå®Ÿè¡Œã‚¹ãƒ†ãƒƒãƒ—:")
            for step, completed in self.steps.items():
                status = "OK" if completed else "NG"
                self.log(f"  {status} {step}")
            
            return True
            
        except Exception as e:
            self.log(f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            return False

    def report_disk_usage(self):
        """ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡ãƒ¬ãƒãƒ¼ãƒˆ"""
        try:
            rakuten_files = list(self.data_dir.glob('rakuten_uchiwai_products_*.json'))
            total_size = sum(f.stat().st_size for f in rakuten_files)
            total_size_mb = total_size / 1024 / 1024
            
            self.log(f"ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡: {total_size_mb:.1f}MB ({len(rakuten_files)}ãƒ•ã‚¡ã‚¤ãƒ«)")
            
            if len(rakuten_files) > 0:
                latest_file = max(rakuten_files, key=lambda x: x.stat().st_mtime)
                latest_size_mb = latest_file.stat().st_size / 1024 / 1024
                self.log(f"æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«: {latest_file.name} ({latest_size_mb:.1f}MB)")
                
        except Exception as e:
            self.log(f"ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}", "WARNING")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(
        description='HAREGift å•†å“ãƒ‡ãƒ¼ã‚¿è‡ªå‹•æ›´æ–°ã‚·ã‚¹ãƒ†ãƒ ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # å®Œå…¨è‡ªå‹•æ›´æ–°ï¼ˆæ¨å¥¨ã€5å€‹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¿æŒï¼‰
  python auto_update_products.py

  # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã§æ›´æ–°ï¼ˆæ¥½å¤©APIå–å¾—ã‚¹ã‚­ãƒƒãƒ—ï¼‰
  python auto_update_products.py --no-fetch

  # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ãªã—ï¼ˆå®Œå…¨è‡ªå‹•ï¼‰
  python auto_update_products.py --no-interactive

  # å–å¾—ä»¶æ•°æŒ‡å®š
  python auto_update_products.py --max-items 10000

  # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¿æŒæ•°æŒ‡å®š
  python auto_update_products.py --keep-backups 10

  # ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ç„¡åŠ¹åŒ–
  python auto_update_products.py --no-cleanup
        """
    )
    
    parser.add_argument(
        '--no-fetch',
        action='store_true',
        help='æ¥½å¤©APIå–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰'
    )
    
    parser.add_argument(
        '--no-interactive',
        action='store_true',
        help='ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå®Œå…¨è‡ªå‹•åŒ–ï¼‰'
    )
    
    parser.add_argument(
        '--max-items',
        type=int,
        default=5000,
        help='æœ€å¤§å–å¾—ä»¶æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5000ï¼‰'
    )
    
    parser.add_argument(
        '--keep-backups',
        type=int,
        default=5,
        help='ä¿æŒã™ã‚‹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰'
    )
    
    parser.add_argument(
        '--no-cleanup',
        action='store_true',
        help='å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•å‰Šé™¤ã‚’ç„¡åŠ¹åŒ–'
    )
    
    args = parser.parse_args()
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ãƒ»å®Ÿè¡Œ
    updater = HAREGiftAutoUpdater(
        no_fetch=args.no_fetch,
        no_interactive=args.no_interactive,
        max_items=args.max_items,
        keep_backups=args.keep_backups,
        cleanup_old=not args.no_cleanup
    )
    
    success = updater.run_full_pipeline()
    sys.exit(0 if success else 1)


if __name__ == '__main__':
    main()