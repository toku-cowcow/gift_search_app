"""
æ¥½å¤©å¸‚å ´APIå•†å“æ¤œç´¢ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒãƒ¬ã®æ—¥ã‚®ãƒ•ãƒˆå•†å“ã‚’æ¤œç´¢ã—ã¦ã€JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹
ã‚¸ãƒ£ãƒ³ãƒ«åå–å¾—æ©Ÿèƒ½ä»˜ãã€ãƒªãƒˆãƒ©ã‚¤ãƒ»å†é–‹æ©Ÿèƒ½å¼·åŒ–ç‰ˆ
"""

import os
import requests
import json
import time
from datetime import datetime
from typing import Dict, List, Any
import random
from dotenv import load_dotenv
from pathlib import Path

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®.envãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
load_dotenv(Path(__file__).parent.parent / '.env')

class RakutenProductFetcher:
    def __init__(self, app_id: str, affiliate_id: str = None):
        self.app_id = app_id
        self.affiliate_id = affiliate_id
        
        # API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
        self.base_url = os.getenv('RAKUTEN_API_ENDPOINT')
        if not self.base_url:
            raise ValueError("RAKUTEN_API_ENDPOINTç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
        self.genre_api_url = os.getenv('RAKUTEN_GENRE_API_ENDPOINT')
        if not self.genre_api_url:
            raise ValueError("RAKUTEN_GENRE_API_ENDPOINTç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
        # ã‚¸ãƒ£ãƒ³ãƒ«åã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆAPIå‘¼ã³å‡ºã—æ•°å‰Šæ¸›ã®ãŸã‚ï¼‰
        self._genre_cache = {}
        self._load_genre_cache()
        
        # é€²æ—ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«
        self.progress_file = os.path.join(os.path.dirname(__file__), 'data', 'fetch_progress.json')
        
    def _load_genre_cache(self):
        """ä¿å­˜ã•ã‚ŒãŸã‚¸ãƒ£ãƒ³ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿"""
        cache_file = os.path.join(os.path.dirname(__file__), 'data', 'genre_cache.json')
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    self._genre_cache = json.load(f)
                print(f"ã‚¸ãƒ£ãƒ³ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿: {len(self._genre_cache)} ä»¶")
            except Exception as e:
                print(f"ã‚¸ãƒ£ãƒ³ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                self._genre_cache = {}
    
    def _save_genre_cache(self):
        """ã‚¸ãƒ£ãƒ³ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        data_dir = os.path.join(os.path.dirname(__file__), 'data')
        os.makedirs(data_dir, exist_ok=True)
        cache_file = os.path.join(data_dir, 'genre_cache.json')
        
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(self._genre_cache, f, ensure_ascii=False, indent=2)
            print(f"ã‚¸ãƒ£ãƒ³ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜: {len(self._genre_cache)} ä»¶")
        except Exception as e:
            print(f"ã‚¸ãƒ£ãƒ³ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _load_progress(self):
        """é€²æ—ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None
    
    def _save_progress(self, page: int, total_items: int, keyword: str):
        """é€²æ—ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        progress = {
            'page': page,
            'total_items': total_items,
            'keyword': keyword,
            'timestamp': datetime.now().isoformat()
        }
        try:
            data_dir = os.path.join(os.path.dirname(__file__), 'data')
            os.makedirs(data_dir, exist_ok=True)
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"é€²æ—ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _retry_request(self, url: str, params: Dict, max_retries: int = 5) -> requests.Response:
        """æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã§HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒªãƒˆãƒ©ã‚¤"""
        for attempt in range(max_retries):
            try:
                response = requests.get(url, params=params, timeout=30)
                response.raise_for_status()
                return response
            except requests.exceptions.RequestException as e:
                if attempt == max_retries - 1:
                    raise e
                
                # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ï¼ˆ1ç§’ã€2ç§’ã€4ç§’ã€8ç§’ã€16ç§’ï¼‰+ ãƒ©ãƒ³ãƒ€ãƒ ã‚¸ãƒƒã‚¿ãƒ¼
                wait_time = (2 ** attempt) + random.uniform(0, 1)
                print(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•— (è©¦è¡Œ {attempt + 1}/{max_retries}): {e}")
                print(f"{wait_time:.1f}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                time.sleep(wait_time)
        
    def get_genre_name(self, genre_id: str) -> str:
        """
        ã‚¸ãƒ£ãƒ³ãƒ«IDã‹ã‚‰ã‚¸ãƒ£ãƒ³ãƒ«åã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰
        
        Args:
            genre_id: æ¥½å¤©ã®ã‚¸ãƒ£ãƒ³ãƒ«ID
            
        Returns:
            ã‚¸ãƒ£ãƒ³ãƒ«åï¼ˆå–å¾—ã§ããªã„å ´åˆã¯ç©ºæ–‡å­—ï¼‰
        """
        if not genre_id:
            return ""
            
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯å³åº§ã«è¿”ã™
        if genre_id in self._genre_cache:
            return self._genre_cache[genre_id]
        
        try:
            params = {
                'applicationId': self.app_id,
                'genreId': genre_id,
                'format': 'json'
            }
            
            response = requests.get(self.genre_api_url, params=params)
            response.raise_for_status()
            
            data = response.json()
            
            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
            if 'error' in data:
                print(f"ã‚¸ãƒ£ãƒ³ãƒ«API ã‚¨ãƒ©ãƒ¼ (ID: {genre_id}): {data['error']}")
                self._genre_cache[genre_id] = ""
                return ""
            
            # ã‚¸ãƒ£ãƒ³ãƒ«æƒ…å ±ã‚’å–å¾—
            if 'current' in data and 'genreName' in data['current']:
                genre_name = data['current']['genreName']
                self._genre_cache[genre_id] = genre_name
                print(f"ã‚¸ãƒ£ãƒ³ãƒ«åå–å¾—: {genre_id} â†’ {genre_name}")
                return genre_name
            else:
                self._genre_cache[genre_id] = ""
                return ""
                
        except Exception as e:
            print(f"ã‚¸ãƒ£ãƒ³ãƒ«åå–å¾—ã‚¨ãƒ©ãƒ¼ (ID: {genre_id}): {e}")
            self._genre_cache[genre_id] = ""
            return ""
        
    def search_products(self, keyword: str, max_items: int = 100, items_per_page: int = 30, resume: bool = False, category_id: str = None) -> List[Dict[str, Any]]:
        """
        æ¥½å¤©å¸‚å ´ã§å•†å“ã‚’æ¤œç´¢ï¼ˆãƒªãƒˆãƒ©ã‚¤ãƒ»å†é–‹æ©Ÿèƒ½ä»˜ãï¼‰
        
        Args:
            keyword: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            max_items: æœ€å¤§å–å¾—ä»¶æ•°
            resume: å‰å›ã®ç¶šãã‹ã‚‰å†é–‹ã™ã‚‹ã‹
        
        Returns:
            å•†å“ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        all_products = []
        page = 1
        items_per_page = 30  # æ¥½å¤©APIã®æœ€å¤§å€¤
        
        # å‰å›ã®é€²æ—ã‚’ç¢ºèª
        if resume:
            progress = self._load_progress()
            if progress and progress.get('keyword') == keyword:
                page = progress.get('page', 1)
                print(f"å‰å›ã®ç¶šãã‹ã‚‰å†é–‹: ãƒšãƒ¼ã‚¸ {page} ã‹ã‚‰é–‹å§‹")
                # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°èª­ã¿è¾¼ã¿
                existing_files = [f for f in os.listdir(os.path.join(os.path.dirname(__file__), 'data')) 
                                if f.startswith('rakuten_uchiwai_products_') and f.endswith('.json')]
                if existing_files:
                    latest_file = sorted(existing_files)[-1]
                    try:
                        with open(os.path.join(os.path.dirname(__file__), 'data', latest_file), 'r', encoding='utf-8') as f:
                            all_products = json.load(f)
                        print(f"æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ {len(all_products)} ä»¶ã‚’èª­ã¿è¾¼ã¿")
                    except Exception as e:
                        print(f"æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                        all_products = []
        
        print(f"æ¥½å¤©å¸‚å ´ã§ã€Œ{keyword}ã€ã®å•†å“ã‚’æ¤œç´¢é–‹å§‹... (ç›®æ¨™: {max_items} ä»¶)")
        
        while len(all_products) < max_items:
            # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            params = {
                'applicationId': self.app_id,
                'keyword': keyword,
                'format': 'json',
                'hits': items_per_page,
                'page': page,
                'sort': 'standard',  # æ¨™æº–é †
                'availability': 1,   # åœ¨åº«ã‚ã‚Š
                'imageFlag': 1,      # ç”»åƒã‚ã‚Š
            }
            
            if self.affiliate_id:
                params['affiliateId'] = self.affiliate_id
            
            try:
                print(f"ãƒšãƒ¼ã‚¸ {page} ã‚’å–å¾—ä¸­...")
                response = self._retry_request(self.base_url, params)
                
                data = response.json()
                
                # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
                if 'error' in data:
                    print(f"API ã‚¨ãƒ©ãƒ¼: {data['error']}")
                    break
                
                items = data.get('Items', [])
                if not items:
                    print("ã“ã‚Œä»¥ä¸Šå•†å“ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                    break
                
                # å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›ï¼ˆcategory_idã‚’æ¸¡ã—ã¦occasionã‚’è¨­å®šï¼‰
                for item_data in items:
                    if len(all_products) >= max_items:
                        break
                    
                    item = item_data['Item']
                    product = self._convert_item_format(item, category_id)
                    
                    # ã‚¸ãƒ£ãƒ³ãƒ«åã‚’å–å¾—ã—ã¦è¿½åŠ ï¼ˆAPIå‘¼ã³å‡ºã—æ•°å‰Šæ¸›ã®ãŸã‚å°‘ã—å¾…æ©Ÿï¼‰
                    if 'genreId' in item:
                        product['genreName'] = self.get_genre_name(str(item['genreId']))
                        time.sleep(0.05)  # ã‚¸ãƒ£ãƒ³ãƒ«APIå‘¼ã³å‡ºã—é–“éš”
                    else:
                        product['genreName'] = ""
                    
                    all_products.append(product)
                
                # ç·ä»¶æ•°ã®è¡¨ç¤ºï¼ˆåˆå›ã®ã¿ï¼‰
                if page == 1:
                    total_count = data.get('count', 0)
                    print(f"ç·ä»¶æ•°: {total_count:,} ä»¶")
                    if total_count > max_items:
                        print(f"ä¸Šé™ {max_items:,} ä»¶ã¾ã§å–å¾—ã—ã¾ã™")
                
                print(f"ç¾åœ¨ {len(all_products):,} ä»¶å–å¾—æ¸ˆã¿")
                
                # é€²æ—ã‚’å®šæœŸä¿å­˜ï¼ˆ10ãƒšãƒ¼ã‚¸ã”ã¨ï¼‰
                if page % 10 == 0:
                    self._save_progress(page + 1, len(all_products), keyword)
                    print(f"é€²æ—ã‚’ä¿å­˜ã—ã¾ã—ãŸ (ãƒšãƒ¼ã‚¸ {page}, {len(all_products)} ä»¶)")
                
                page += 1
                
                # APIåˆ¶é™ã‚’è€ƒæ…®ã—ã¦å°‘ã—å¾…æ©Ÿ
                time.sleep(0.2)  # å°‘ã—é•·ã‚ã«
                
            except requests.exceptions.RequestException as e:
                print(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                print("ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ã§æ—¢ã«å‡¦ç†æ¸ˆã¿ã§ã™ã€‚")
                break
            except Exception as e:
                print(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
                break
        
        print(f"å–å¾—å®Œäº†: {len(all_products):,} ä»¶")
        
        # æœ€çµ‚é€²æ—ã‚’ä¿å­˜
        self._save_progress(page, len(all_products), keyword)
        
        # ã‚¸ãƒ£ãƒ³ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
        self._save_genre_cache()
        
        return all_products
    
    def _convert_item_format(self, item: Dict, category_id: str = None) -> Dict[str, Any]:
        """
        æ¥½å¤©APIã®å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’å†…éƒ¨å½¢å¼ã«å¤‰æ›
        """
        import html
        
        # å•†å“IDã®ç”Ÿæˆï¼ˆæ¥½å¤©ã®å•†å“ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã€ã‚³ãƒ­ãƒ³ã‚’ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã«å¤‰æ›ï¼‰
        item_code = item.get('itemCode', 'unknown').replace(':', '_')
        item_id = f"rakuten_{item_code}"
        
        # ä¾¡æ ¼ã®å–å¾—
        price = item.get('itemPrice', 0)
        
        # ç”»åƒURLã®å–å¾—ï¼ˆè¤‡æ•°ã‚ã‚‹å ´åˆã¯æœ€åˆã®ã‚‚ã®ï¼‰
        image_url = ""
        if 'mediumImageUrls' in item and item['mediumImageUrls']:
            image_url = item['mediumImageUrls'][0]['imageUrl']
        elif 'smallImageUrls' in item and item['smallImageUrls']:
            image_url = item['smallImageUrls'][0]['imageUrl']
        
        # ãƒ¬ãƒ“ãƒ¥ãƒ¼æƒ…å ±
        review_count = item.get('reviewCount', 0)
        review_average = item.get('reviewAverage', 0)
        
        # å•†å“èª¬æ˜ï¼ˆHTMLã‚¿ã‚°ã‚’å«ã‚€å ´åˆãŒã‚ã‚‹ã®ã§æ³¨æ„ï¼‰
        item_caption = html.unescape(item.get('itemCaption', ''))
        
        # å•†å“åã®HTMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        item_name = html.unescape(item.get('itemName', ''))
        shop_name = html.unescape(item.get('shopName', ''))
        catch_copy = html.unescape(item.get('catchcopy', ''))
        
        # ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆURLï¼ˆè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
        affiliate_url = item.get('affiliateUrl', item.get('itemUrl', ''))
        
        # ã‚¸ãƒ£ãƒ³ãƒ«IDã‚’ä¿å­˜ï¼ˆã‚¸ãƒ£ãƒ³ãƒ«åå–å¾—ç”¨ï¼‰
        genre_id = item.get('genreId', '')
        
        return {
            'id': item_id,
            'title': item_name,
            'price': price,
            'image_url': image_url,
            'merchant': shop_name,
            'source': 'rakuten',
            'url': item.get('itemUrl', ''),
            'affiliate_url': affiliate_url,
            'occasion': category_id or 'unknown',  # ã‚«ãƒ†ã‚´ãƒªã«åŸºã¥ã„ã¦è¨­å®š
            'updated_at': int(datetime.now().timestamp()),
            # è¿½åŠ æƒ…å ±
            'review_count': review_count,
            'review_average': review_average,
            'description': item_caption,
            'shop_code': item.get('shopCode', ''),
            'item_code': item.get('itemCode', ''),
            'catch_copy': catch_copy,
            'tags': item.get('tagIds', []),
            'genre_id': genre_id,
            # genreNameã¯å¾Œã§è¿½åŠ ã•ã‚Œã‚‹
        }
    
    def save_to_json(self, products: List[Dict], filename: str):
        """
        å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ : sources/rakuten/ ã«ä¿å­˜
        """
        # æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ : sources/rakuten/ã«ä¿å­˜
        base_dir = os.path.dirname(__file__)
        data_dir = os.path.join(base_dir, 'data', 'sources', 'rakuten')
        os.makedirs(data_dir, exist_ok=True)
        
        filepath = os.path.join(data_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(products, f, ensure_ascii=False, indent=2)
        
        print(f"ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {os.path.getsize(filepath) / 1024 / 1024:.2f} MB")

def main():
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å€¤ã‚’èª­ã¿è¾¼ã¿
    try:
        from config import RAKUTEN_APP_ID, RAKUTEN_AFFILIATE_ID, SEARCH_CATEGORIES
        APP_ID = RAKUTEN_APP_ID
        AFFILIATE_ID = RAKUTEN_AFFILIATE_ID
    except ImportError:
        print("ã‚¨ãƒ©ãƒ¼: config.pyãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # APIã‚­ãƒ¼ã®ç¢ºèª
    if APP_ID == "YOUR_APP_ID_HERE" or not APP_ID:
        print("ã‚¨ãƒ©ãƒ¼: æ¥½å¤©ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³IDã‚’è¨­å®šã—ã¦ãã ã•ã„")
        print("config.pyã® RAKUTEN_APP_ID ã‚’å®Ÿéš›ã®IDã«å¤‰æ›´ã—ã¦ãã ã•ã„")
        return
    
    print("ğŸ HAREGift ã‚«ãƒ†ã‚´ãƒªåˆ¥å•†å“æ¤œç´¢ã‚’é–‹å§‹ã—ã¾ã™...")
    print("=" * 60)
    
    fetcher = RakutenProductFetcher(APP_ID, AFFILIATE_ID)
    all_products = []
    category_stats = {}
    
    # å„ã‚«ãƒ†ã‚´ãƒªã§æ¤œç´¢å®Ÿè¡Œ
    for category_id, config in SEARCH_CATEGORIES.items():
        print(f"\nğŸ“‚ ã‚«ãƒ†ã‚´ãƒª: {category_id}")
        print(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(config['keywords'])}")
        print(f"ğŸ¯ ç›®æ¨™ä»¶æ•°: {config['max_items']}ä»¶")
        
        category_products = []
        
        # å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢
        for keyword in config['keywords']:
            print(f"   æ¤œç´¢ä¸­: ã€Œ{keyword}ã€...")
            
            # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®æœ€å¤§ä»¶æ•°ã‚’å‡ç­‰åˆ†å‰²
            max_per_keyword = config['max_items'] // len(config['keywords'])
            
            products = fetcher.search_products(
                keyword, 
                max_items=max_per_keyword, 
                resume=True,
                category_id=category_id  # ã‚«ãƒ†ã‚´ãƒªIDã‚’æ¸¡ã™
            )
            
            if products:
                # å„å•†å“ã«ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’è¿½åŠ 
                for product in products:
                    product['category_group'] = category_id
                    product['search_keyword'] = keyword
                
                category_products.extend(products)
                print(f"     å–å¾—ä»¶æ•°: {len(products)}ä»¶")
            
            # APIåˆ¶é™å¯¾ç­–ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–“ã§å°‘ã—å¾…æ©Ÿï¼‰
            time.sleep(1)
        
        # é‡è¤‡é™¤å»ï¼ˆå•†å“IDãƒ™ãƒ¼ã‚¹ï¼‰
        seen_ids = set()
        unique_products = []
        for product in category_products:
            if product['id'] not in seen_ids:
                seen_ids.add(product['id'])
                unique_products.append(product)
        
        category_stats[category_id] = len(unique_products)
        all_products.extend(unique_products)
        
        print(f"âœ… {category_id}: {len(unique_products)}ä»¶ï¼ˆé‡è¤‡é™¤å»å¾Œï¼‰")
        print("-" * 40)
    
    print(f"\nğŸ‰ å…¨ã‚«ãƒ†ã‚´ãƒªæ¤œç´¢å®Œäº†ï¼")
    print("=" * 60)
    
    if all_products:
        # ãƒ•ã‚¡ã‚¤ãƒ«åã«æ—¥ä»˜ã‚’å«ã‚ã‚‹
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'rakuten_haregift_products_{timestamp}.json'
        
        fetcher.save_to_json(all_products, filename)
        
        # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
        print("=== ğŸ“Š å–å¾—ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ ===")
        print(f"ç·å•†å“æ•°: {len(all_products):,} ä»¶")
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
        print("\nğŸ“‹ ã‚«ãƒ†ã‚´ãƒªåˆ¥ä»¶æ•°:")
        for category_id, count in category_stats.items():
            percentage = (count / len(all_products)) * 100
            print(f"  {category_id}: {count:,}ä»¶ ({percentage:.1f}%)")
        
        if all_products:
            prices = [p['price'] for p in all_products if p['price'] > 0]
            if prices:
                print(f"\nğŸ’° ä¾¡æ ¼å¸¯: {min(prices):,}å†† ã€œ {max(prices):,}å††")
                print(f"ğŸ’° å¹³å‡ä¾¡æ ¼: {sum(prices)/len(prices):,.0f}å††")
            
            # ãƒ¬ãƒ“ãƒ¥ãƒ¼çµ±è¨ˆ
            reviewed_items = [p for p in all_products if p['review_count'] > 0]
            if reviewed_items:
                avg_rating = sum(p['review_average'] for p in reviewed_items) / len(reviewed_items)
                total_reviews = sum(p['review_count'] for p in reviewed_items)
                print(f"ãƒ¬ãƒ“ãƒ¥ãƒ¼ä»˜ãå•†å“: {len(reviewed_items):,} ä»¶")
                print(f"å¹³å‡è©•ä¾¡: {avg_rating:.2f}ç‚¹")
                print(f"ç·ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°: {total_reviews:,} ä»¶")
        
        # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®æ¡ˆå†…
        print("\n=== æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ— ===")
        print("1. ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        print("2. Meilisearchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ç™»éŒ²ã™ã‚‹å ´åˆ:")
        print("   python update_meilisearch_from_rakuten.py")
        
        # ã‚¸ãƒ£ãƒ³ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®çµ±è¨ˆ
        if hasattr(fetcher, '_genre_cache'):
            print(f"\n=== ã‚¸ãƒ£ãƒ³ãƒ«æƒ…å ± ===")
            print(f"å–å¾—ã—ãŸã‚¸ãƒ£ãƒ³ãƒ«æ•°: {len(fetcher._genre_cache)} ç¨®é¡")
            unique_genres = set(g for g in fetcher._genre_cache.values() if g)
            if unique_genres:
                print("å–å¾—ã‚¸ãƒ£ãƒ³ãƒ«ä¾‹:")
                for genre in sorted(list(unique_genres))[:10]:  # æœ€åˆã®10å€‹ã‚’è¡¨ç¤º
                    print(f"  - {genre}")
                if len(unique_genres) > 10:
                    print(f"  ... ä»– {len(unique_genres) - 10} ç¨®é¡")
    else:
        print("å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

if __name__ == "__main__":
    main()