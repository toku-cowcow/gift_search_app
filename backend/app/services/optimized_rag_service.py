"""
Phase 3: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ç‰ˆ LangChain RAGã‚µãƒ¼ãƒ“ã‚¹

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¹å‰²:
- Phase 2ã®æ©Ÿèƒ½ã‚’ä¿æŒã—ã¤ã¤ã€å¿œç­”æ™‚é–“ã‚’å¤§å¹…çŸ­ç¸®
- ä¸¦åˆ—å‡¦ç†ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ã®å®Ÿè£…
- 3-5ç§’ä»¥å†…ã®å¿œç­”æ™‚é–“ã‚’ç›®æŒ‡ã™
"""

import asyncio
import json
import logging
import hashlib
import time
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor

# LangChain imports
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS

from ..core.config import settings
from ..schemas import GiftItem, SearchParams
from .search_service_fixed import MeilisearchService
from .hybrid_search_engine import HybridSearchEngine

# ãƒ­ã‚°è¨­å®š
logger = logging.getLogger(__name__)


class PerformanceOptimizer:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"""
    
    def __init__(self):
        self.cache = {}
        self.cache_ttl = timedelta(minutes=30)  # 30åˆ†ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.executor = ThreadPoolExecutor(max_workers=4)
    
    def get_cache_key(self, data: Any) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
        return hashlib.md5(str(data).encode()).hexdigest()
    
    def get_cached(self, key: str) -> Optional[Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        if key in self.cache:
            value, timestamp = self.cache[key]
            if datetime.now() - timestamp < self.cache_ttl:
                return value
            else:
                del self.cache[key]
        return None
    
    def set_cached(self, key: str, value: Any):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        self.cache[key] = (value, datetime.now())
    
    def clear_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢"""
        self.cache.clear()


class OptimizedUserIntentExtractor:
    """æœ€é©åŒ–ç‰ˆæ„å›³æŠ½å‡ºå™¨"""
    
    def __init__(self, llm: ChatOpenAI, optimizer: PerformanceOptimizer):
        self.llm = llm
        self.optimizer = optimizer
        self._setup_optimized_prompt()
    
    def _setup_optimized_prompt(self):
        """æœ€é©åŒ–ã•ã‚ŒãŸç°¡æ½”ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""            
        self.intent_prompt = ChatPromptTemplate.from_messages([
            ("system", """
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‹ã‚‰æ„å›³ã‚’æŠ½å‡ºã—ã€JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚

ä¾‹ï¼š
å…¥åŠ›: "ç”¨é€”: çµå©šå†…ç¥ã„ã€ç›¸æ‰‹: ä¸Šå¸ãƒ»ç›®ä¸Šã®æ–¹ã€äºˆç®—: 3000å††ã€œ5000å††"
å‡ºåŠ›: {"occasion":"wedding_return","target_relationship":"boss","budget_min":3000,"budget_max":5000,"keywords":["ä¸Šå“","å†…ç¥ã„"]}

äºˆç®—ã®æŠ½å‡ºè¦å‰‡:
- "3000å††ã€œ5000å††" â†’ budget_min:3000, budget_max:5000
- "5000å††ãã‚‰ã„" â†’ budget_min:4000, budget_max:6000  
- "1ä¸‡å††ä»¥ä¸‹" â†’ budget_min:null, budget_max:10000
- "äºˆç®—ãªã—" â†’ budget_min:null, budget_max:null

occasion: wedding_return(çµå©šå†…ç¥ã„), baby_return(å‡ºç”£å†…ç¥ã„), funeral_return(é¦™å…¸è¿”ã—), celebration_return(ãŠç¥ã„è¿”ã—), business(ãƒ“ã‚¸ãƒã‚¹é–¢ä¿‚), other(ãã®ä»–), unknown
target_relationship: boss(ä¸Šå¸ãƒ»ç›®ä¸Šã®æ–¹), colleague(åŒåƒš), friend(å‹äºº), family(è¦ªæ—), client(å–å¼•å…ˆ), other(ãã®ä»–), unknown  
budget_min/max: æ•°å€¤ã¾ãŸã¯null
keywords: é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é…åˆ—

JSONã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚èª¬æ˜ã‚„å‰æ›¸ãã¯ä¸è¦ã§ã™ã€‚
"""),
            ("user", "{user_input}")
        ])
    
    async def extract_intent(self, user_input: str) -> Dict[str, Any]:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãé«˜é€Ÿæ„å›³æŠ½å‡º
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèªï¼ˆä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ï¼‰
        # cache_key = self.optimizer.get_cache_key(f"intent_{user_input}")
        # cached_result = self.optimizer.get_cached(cache_key)
        
        # if cached_result:
        #     logger.info(f"æ„å›³æŠ½å‡ºã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {user_input[:50]}...")
        #     return cached_result
        
        try:
            start_time = time.time()
            
            logger.info(f"ğŸ” æ„å›³æŠ½å‡ºé–‹å§‹: {user_input}")
            
            response = await self.llm.ainvoke(
                self.intent_prompt.format_messages(user_input=user_input)
            )
            
            # æ”¹å–„ã•ã‚ŒãŸJSONè§£æ
            content = response.content.strip()
            logger.info(f"ğŸ¤– LLMç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹: {content}")
            
            # JSONãƒ–ãƒ­ãƒƒã‚¯ãƒãƒ¼ã‚«ãƒ¼ã‚’é™¤å»
            if content.startswith('```'):
                content = content.strip('`')
                if content.startswith('json\n'):
                    content = content[5:]  # "json\n"ã‚’å‰Šé™¤
                elif content.startswith('json'):
                    content = content[4:]  # "json"ã‚’å‰Šé™¤
                content = content.strip()
            
            # JSONä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»ï¼ˆæœ€åˆã®{ã‹ã‚‰æœ€å¾Œã®}ã¾ã§æŠ½å‡ºï¼‰
            start_idx = content.find('{')
            end_idx = content.rfind('}') + 1
            if start_idx >= 0 and end_idx > start_idx:
                content = content[start_idx:end_idx]
            
            logger.info(f"ğŸ”§ å‡¦ç†å¾ŒJSON: {content}")
            
            # Pythonã£ã½ã„è¨˜æ³•ã‚’JSONå½¢å¼ã«å¤‰æ›
            content = content.replace("'", '"')  # å˜ä¸€å¼•ç”¨ç¬¦ã‚’äºŒé‡å¼•ç”¨ç¬¦ã«
            content = content.replace('None', 'null')  # Noneã‚’nullã«
            content = content.replace('True', 'true')  # Trueã‚’trueã«
            content = content.replace('False', 'false')  # Falseã‚’falseã«
            
            intent = json.loads(content)
            logger.info(f"âœ… ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿æ„å›³: {intent}")
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è£œå®Œ
            intent = self._normalize_intent(intent)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ï¼‰
            # self.optimizer.set_cached(cache_key, intent)
            
            elapsed = time.time() - start_time
            logger.info(f"æ„å›³æŠ½å‡ºå®Œäº†: {elapsed:.2f}s, {intent}")
            
            return intent
            
        except Exception as e:
            logger.warning(f"æ„å›³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return self._get_default_intent()
    
    def _normalize_intent(self, intent: Dict[str, Any]) -> Dict[str, Any]:
        """æ„å›³ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–ï¼ˆé«˜é€Ÿç‰ˆï¼‰"""
        return {
            "occasion": intent.get("occasion", "unknown"),
            "target_relationship": intent.get("target_relationship", "unknown"),
            "budget_min": intent.get("budget_min"),
            "budget_max": intent.get("budget_max"),
            "keywords": intent.get("keywords", []) if isinstance(intent.get("keywords"), list) else [],
            "gender": intent.get("gender", "unknown"),
            "urgency": intent.get("urgency", "normal")
        }
    
    def _get_default_intent(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ„å›³ï¼ˆæœ€å°é™ï¼‰"""
        return {
            "occasion": "unknown",
            "target_relationship": "unknown",
            "budget_min": None,
            "budget_max": None,
            "keywords": [],
            "gender": "unknown",
            "urgency": "normal"
        }


class OptimizedLangChainRAGService:
    """Phase 3: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ç‰ˆRAGã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.meilisearch_service = MeilisearchService()
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.optimizer = PerformanceOptimizer()
        
        # æœ€é©åŒ–ã•ã‚ŒãŸLLMè¨­å®š
        self.llm = ChatOpenAI(
            model_name=settings.openai_model,
            temperature=0.1,  # ä½æ¸©åº¦ã§é«˜é€ŸåŒ–
            max_tokens=800,   # ãƒˆãƒ¼ã‚¯ãƒ³æ•°åˆ¶é™
            api_key=settings.openai_api_key
        )
        
        self.embeddings = OpenAIEmbeddings(api_key=settings.openai_api_key)
        
        # æœ€é©åŒ–ã•ã‚ŒãŸæ„å›³æŠ½å‡ºå™¨
        self.intent_extractor = OptimizedUserIntentExtractor(self.llm, self.optimizer)
        
        # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ï¼ˆè»½é‡ç‰ˆï¼‰
        self.vector_store = None
        self.hybrid_engine = None
        
        # åˆæœŸåŒ–
        self._initialize_optimized_components()
    
    def _initialize_optimized_components(self):
        """æœ€é©åŒ–ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–"""
        try:
            # è»½é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢èª­ã¿è¾¼ã¿
            import os
            vector_store_path = os.path.join(
                os.path.dirname(__file__), 
                "../../../data/vector_store"
            )
            
            if os.path.exists(vector_store_path):
                self.vector_store = FAISS.load_local(
                    vector_store_path, 
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                logger.info("âœ… æœ€é©åŒ–ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢èª­ã¿è¾¼ã¿å®Œäº†")
            
            # æœ€é©åŒ–ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¨ãƒ³ã‚¸ãƒ³
            if self.vector_store:
                self.hybrid_engine = HybridSearchEngine(
                    meilisearch_service=self.meilisearch_service,
                    vector_store=self.vector_store
                )
                # æ¤œç´¢é‡ã¿ã‚’é«˜é€ŸåŒ–å‘ã‘ã«èª¿æ•´
                self.hybrid_engine.search_weights = {
                    "semantic_weight": 0.7,  # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯é‡è¦–
                    "structured_weight": 0.3,
                    "intent_boost": 0.15
                }
                logger.info("âœ… æœ€é©åŒ–ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
            
        except Exception as e:
            logger.error(f"æœ€é©åŒ–ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    async def get_fast_recommendation(
        self,
        user_input: str,
        chat_history: List[Dict[str, str]] = None,
        limit: int = 3  # æ¨è–¦æ•°ã‚’å‰Šæ¸›
    ) -> Dict[str, Any]:
        """
        Phase 3: é«˜é€Ÿæ¨è–¦ãƒ¡ã‚¤ãƒ³å‡¦ç†
        
        ç›®æ¨™: 3-5ç§’ä»¥å†…ã§ã®å¿œç­”
        """
        start_time = datetime.now()
        processing_steps = []
        
        try:
            logger.info("Phase 3: é«˜é€Ÿæ¨è–¦é–‹å§‹")
            
            # Step 1: æ„å›³æŠ½å‡ºï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãä¸¦åˆ—å®Ÿè¡Œæº–å‚™ï¼‰
            intent_task = asyncio.create_task(
                self.intent_extractor.extract_intent(user_input)
            )
            
            # Step 2: ä¸¦åˆ—ã§ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢æº–å‚™
            if self.hybrid_engine:
                # æ„å›³æŠ½å‡ºå®Œäº†ã‚’å¾…ã¤
                user_intent = await intent_task
                processing_steps.append(f"é«˜é€Ÿæ„å›³æŠ½å‡ºå®Œäº†")
                
                # Step 3: æœ€é©åŒ–ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢
                search_start = time.time()
                hybrid_results, search_metadata = await self._fast_hybrid_search(
                    query=user_input,
                    user_intent=user_intent,
                    limit=limit
                )
                search_time = time.time() - search_start
                processing_steps.append(f"é«˜é€Ÿãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢: {search_time:.2f}s")
                
                # Step 4: é«˜é€ŸAIå¿œç­”ç”Ÿæˆ
                response_start = time.time()
                ai_response = await self._generate_fast_response(
                    user_input=user_input,
                    user_intent=user_intent,
                    recommended_products=hybrid_results
                )
                response_time = time.time() - response_start
                processing_steps.append(f"é«˜é€Ÿå¿œç­”ç”Ÿæˆ: {response_time:.2f}s")
                
                total_time = (datetime.now() - start_time).total_seconds()
                
                return {
                    "ai_response": ai_response,
                    "recommendations": hybrid_results,
                    "user_intent": user_intent,
                    "search_metadata": search_metadata,
                    "processing_steps": processing_steps,
                    "performance": {
                        "total_time_ms": total_time * 1000,
                        "search_time_ms": search_time * 1000,
                        "response_time_ms": response_time * 1000,
                        "optimization": "phase3_fast"
                    },
                    "reasoning": "Phase 3é«˜é€Ÿæœ€é©åŒ–ï¼ˆä¸¦åˆ—å‡¦ç† + ã‚­ãƒ£ãƒƒã‚·ãƒ¥ + è»½é‡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰"
                }
            
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                user_intent = await intent_task
                return await self._fallback_fast_search(user_input, user_intent, limit)
                
        except Exception as e:
            logger.error(f"é«˜é€Ÿæ¨è–¦ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return await self._emergency_response(user_input)
    
    async def _fast_hybrid_search(
        self,
        query: str,
        user_intent: Dict[str, Any],
        limit: int
    ) -> Tuple[List[GiftItem], Dict[str, Any]]:
        """æœ€é©åŒ–ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢"""
        try:
            # æ¤œç´¢ç¯„å›²ã‚’åˆ¶é™ï¼ˆé«˜é€ŸåŒ–ï¼‰
            limited_limit = min(limit * 2, 6)  # æœ€å¤§6ä»¶ã¾ã§
            
            results, metadata = await self.hybrid_engine.hybrid_search(
                query=query,
                user_intent=user_intent,
                limit=limited_limit,
                semantic_threshold=0.6  # é–¾å€¤ã‚’ä¸‹ã’ã¦é«˜é€ŸåŒ–
            )
            
            return results[:limit], metadata
            
        except Exception as e:
            logger.error(f"é«˜é€Ÿãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return [], {"error": str(e), "fallback": True}
    
    async def _generate_fast_response(
        self,
        user_input: str,
        user_intent: Dict[str, Any],
        recommended_products: List[GiftItem]
    ) -> str:
        """é«˜é€ŸAIå¿œç­”ç”Ÿæˆï¼ˆç°¡æ½”ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç‰ˆï¼‰"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
        response_key = self.optimizer.get_cache_key(
            f"response_{user_input}_{len(recommended_products)}"
        )
        cached_response = self.optimizer.get_cached(response_key)
        
        if cached_response:
            logger.info("AIå¿œç­”ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ")
            return cached_response
        
        try:
            # ç°¡æ½”ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            if recommended_products:
                products_text = "\n".join([
                    f"{i+1}. {p.title} - {p.price:,}å††"
                    for i, p in enumerate(recommended_products[:3])
                ])
            else:
                products_text = "è©²å½“ã™ã‚‹å•†å“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            
            fast_prompt = f"""
å†…ç¥ã„ã‚®ãƒ•ãƒˆã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã¨ã—ã¦ã€ç°¡æ½”ã«æ¨è–¦ã—ã¦ãã ã•ã„ã€‚

è¦æœ›: {user_input}

æ¨è–¦å•†å“:
{products_text}

ç”¨é€”: {user_intent.get('occasion', 'ä¸æ˜')}
ç›¸æ‰‹: {user_intent.get('target_relationship', 'ä¸æ˜')}

ä»¥ä¸‹ã®å½¢å¼ã§å›ç­”ï¼ˆ200æ–‡å­—ä»¥å†…ï¼‰:
1. æ¨è–¦ç†ç”±
2. ãŠã™ã™ã‚å•†å“ï¼ˆä¸Šä½2ã¤ï¼‰
3. ä¸€è¨€ã‚¢ãƒ‰ãƒã‚¤ã‚¹
"""
            
            response = await self.llm.ainvoke(fast_prompt)
            result = response.content
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.optimizer.set_cached(response_key, result)
            
            return result
            
        except Exception as e:
            logger.error(f"é«˜é€Ÿå¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return self._get_emergency_text(user_input, recommended_products)
    
    async def _fallback_fast_search(
        self,
        user_input: str,
        user_intent: Dict[str, Any],
        limit: int
    ) -> Dict[str, Any]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é«˜é€Ÿæ¤œç´¢"""
        try:
            # æ§‹é€ åŒ–æ¤œç´¢ã®ã¿ã§é«˜é€ŸåŒ–
            search_params = SearchParams(
                q=user_input,
                occasion=user_intent.get('occasion') if user_intent.get('occasion') != 'unknown' else None,
                price_min=user_intent.get('budget_min'),
                price_max=user_intent.get('budget_max'),
                limit=limit
            )
            
            search_result = self.meilisearch_service.search_items(search_params)
            
            # ç°¡å˜ãªå¿œç­”ç”Ÿæˆ
            simple_response = f"ã€Œ{user_input}ã€ã«å¯¾ã—ã¦{len(search_result.hits)}ä»¶ã®å•†å“ã‚’ã”ææ¡ˆã„ãŸã—ã¾ã™ã€‚"
            
            return {
                "ai_response": simple_response,
                "recommendations": search_result.hits,
                "user_intent": user_intent,
                "search_metadata": {"strategy": "fallback_structured_only"},
                "processing_steps": ["ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ§‹é€ åŒ–æ¤œç´¢"],
                "reasoning": "é«˜é€Ÿãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢"
            }
            
        except Exception as e:
            logger.error(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return await self._emergency_response(user_input)
    
    async def _emergency_response(self, user_input: str) -> Dict[str, Any]:
        """ç·Šæ€¥æ™‚ã®æœ€å°å¿œç­”"""
        return {
            "ai_response": f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã€Œ{user_input}ã€ã«ã¤ã„ã¦ã®ã”è¦æœ›ã‚’æ‰¿ã‚Šã¾ã—ãŸã€‚å°‘ã€…ãŠæ™‚é–“ã‚’ã„ãŸã ã„ã¦ã€æœ€é©ãªå•†å“ã‚’ãŠæ¢ã—ã„ãŸã—ã¾ã™ã€‚",
            "recommendations": [],
            "user_intent": {"occasion": "unknown"},
            "search_metadata": {"strategy": "emergency"},
            "processing_steps": ["ç·Šæ€¥å¿œç­”"],
            "reasoning": "ç·Šæ€¥æ™‚æœ€å°å¿œç­”"
        }
    
    def _get_emergency_text(self, user_input: str, products: List[GiftItem]) -> str:
        """ç·Šæ€¥æ™‚ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        if products:
            return f"ã€Œ{user_input}ã€ã®ã”è¦æœ›ã«ãŠå¿œãˆã—ã¦ã€{len(products)}ä»¶ã®å•†å“ã‚’ã”ææ¡ˆã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚ã”æ¤œè¨ãã ã•ã„ã€‚"
        else:
            return f"ã€Œ{user_input}ã€ã«åˆã†å•†å“ã‚’ãŠæ¢ã—ã—ã¦ãŠã‚Šã¾ã™ã€‚åˆ¥ã®æ¡ä»¶ã§ã‚‚ãŠæ°—è»½ã«ãŠå£°ãŒã‘ãã ã•ã„ã€‚"
    
    async def health_check(self) -> Dict[str, Any]:
        """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
        return {
            "status": "healthy",
            "optimization": "phase3",
            "cache_size": len(self.optimizer.cache),
            "hybrid_engine_ready": self.hybrid_engine is not None,
            "vector_store_ready": self.vector_store is not None
        }