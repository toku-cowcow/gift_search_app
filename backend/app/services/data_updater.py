"""
ãƒ‡ãƒ¼ã‚¿è‡ªå‹•æ›´æ–°ã‚·ã‚¹ãƒ†ãƒ 

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¹å‰²:
- æœ€æ–°ã®rakuten_uchiwai_products_*.jsonãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•é¸æŠ
- ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã€Meilisearchã®å·®åˆ†æ›´æ–°
- å£Šã‚ŒãŸãƒ¬ã‚³ãƒ¼ãƒ‰ã®æ¤œå‡ºãƒ»ãƒ­ã‚°å‡ºåŠ›
- æ—¥æ¬¡å®Ÿè¡Œã®ãŸã‚ã®ã‚¸ãƒ§ãƒ–æ©Ÿèƒ½
"""

import os
import json
import logging
import re
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path

from ..schemas import GiftItem
from .search_service_fixed import MeilisearchService
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

# ãƒ­ã‚°è¨­å®š
logger = logging.getLogger(__name__)


class DataUpdater:
    """ãƒ‡ãƒ¼ã‚¿è‡ªå‹•æ›´æ–°ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(
        self,
        data_dir: str = "scripts/data",
        meilisearch_service: Optional[MeilisearchService] = None,
        vector_store: Optional[FAISS] = None,
        embeddings: Optional[OpenAIEmbeddings] = None
    ):
        """
        åˆæœŸåŒ–
        
        Args:
            data_dir: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            meilisearch_service: Meilisearchã‚µãƒ¼ãƒ“ã‚¹
            vector_store: FAISSãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢
            embeddings: åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
        """
        self.data_dir = Path(data_dir)
        self.meilisearch_service = meilisearch_service
        self.vector_store = vector_store
        self.embeddings = embeddings
        
        # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãƒ«ãƒ¼ãƒ«
        self.required_fields = ['id', 'title', 'price', 'url', 'merchant']
        self.price_range = (100, 1000000)  # 100å††ã€œ100ä¸‡å††
        
        logger.info("ãƒ‡ãƒ¼ã‚¿æ›´æ–°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def find_latest_data_file(self) -> Optional[Path]:
        """
        æœ€æ–°ã®rakuten_uchiwai_products_*.jsonãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        
        Returns:
            æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯None
        """
        try:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
            pattern = r'rakuten_uchiwai_products_(\d{8})\.json'
            latest_date = None
            latest_file = None
            
            for file_path in self.data_dir.glob('rakuten_uchiwai_products_*.json'):
                match = re.match(pattern, file_path.name)
                if match:
                    date_str = match.group(1)
                    try:
                        file_date = datetime.strptime(date_str, '%Y%m%d')
                        if latest_date is None or file_date > latest_date:
                            latest_date = file_date
                            latest_file = file_path
                    except ValueError:
                        logger.warning(f"ç„¡åŠ¹ãªæ—¥ä»˜å½¢å¼ãƒ•ã‚¡ã‚¤ãƒ«: {file_path.name}")
                        continue
            
            if latest_file:
                logger.info(f"æœ€æ–°ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹: {latest_file.name} ({latest_date.strftime('%Y-%m-%d')})")
                return latest_file
            else:
                logger.warning(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.data_dir}")
                return None
                
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def validate_record(self, record: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """
        ãƒ¬ã‚³ãƒ¼ãƒ‰ã®å¦¥å½“æ€§æ¤œè¨¼
        
        Args:
            record: æ¤œè¨¼ã™ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰
            
        Returns:
            (æœ‰åŠ¹ãƒ•ãƒ©ã‚°, ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ)
        """
        errors = []
        
        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒã‚§ãƒƒã‚¯
        for field in self.required_fields:
            if field not in record or not record[field]:
                errors.append(f"å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ¬ è½: {field}")
        
        # ä¾¡æ ¼ç¯„å›²ãƒã‚§ãƒƒã‚¯
        price = record.get('price')
        if price is not None:
            try:
                price_num = float(price)
                if not (self.price_range[0] <= price_num <= self.price_range[1]):
                    errors.append(f"ç•°å¸¸ãªä¾¡æ ¼: {price_num}")
            except (ValueError, TypeError):
                errors.append(f"ç„¡åŠ¹ãªä¾¡æ ¼å½¢å¼: {price}")
        
        # URLå½¢å¼ãƒã‚§ãƒƒã‚¯
        url = record.get('url', '')
        if url and not (url.startswith('http://') or url.startswith('https://')):
            errors.append(f"ç„¡åŠ¹ãªURLå½¢å¼: {url}")
        
        # IDé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆåŸºæœ¬çš„ãªå½¢å¼ãƒã‚§ãƒƒã‚¯ï¼‰
        record_id = record.get('id', '')
        if not record_id or len(record_id) < 5:
            errors.append(f"ç„¡åŠ¹ãªID: {record_id}")
        
        return len(errors) == 0, errors
    
    def load_and_validate_data(self, file_path: Path) -> Tuple[List[Dict], List[Dict]]:
        """
        ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¨æ¤œè¨¼
        
        Args:
            file_path: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            (æœ‰åŠ¹ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ, ç„¡åŠ¹ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ)
        """
        valid_records = []
        invalid_records = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {len(data)}ä»¶")
            
            for i, record in enumerate(data):
                is_valid, errors = self.validate_record(record)
                
                if is_valid:
                    valid_records.append(record)
                else:
                    invalid_record = {
                        'index': i,
                        'record': record,
                        'errors': errors
                    }
                    invalid_records.append(invalid_record)
            
            logger.info(f"ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å®Œäº†: æœ‰åŠ¹ {len(valid_records)}ä»¶, ç„¡åŠ¹ {len(invalid_records)}ä»¶")
            
            # ç„¡åŠ¹ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ãƒ­ã‚°å‡ºåŠ›
            if invalid_records:
                logger.warning(f"ç„¡åŠ¹ãƒ¬ã‚³ãƒ¼ãƒ‰æ¤œå‡º: {len(invalid_records)}ä»¶")
                for invalid in invalid_records[:10]:  # æœ€åˆã®10ä»¶ã®ã¿è©³ç´°ãƒ­ã‚°
                    logger.warning(f"  è¡Œ{invalid['index']}: {invalid['errors']}")
                    
            return valid_records, invalid_records
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return [], []
    
    async def update_meilisearch(self, records: List[Dict]) -> bool:
        """
        Meilisearchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ›´æ–°
        
        Args:
            records: æ›´æ–°ã™ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰
            
        Returns:
            æ›´æ–°æˆåŠŸãƒ•ãƒ©ã‚°
        """
        if not self.meilisearch_service:
            logger.warning("Meilisearchã‚µãƒ¼ãƒ“ã‚¹ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
        
        try:
            # ãƒãƒƒãƒæ›´æ–°ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š
            batch_size = 1000
            total_updated = 0
            
            for i in range(0, len(records), batch_size):
                batch = records[i:i + batch_size]
                
                # GiftItemã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
                gift_items = []
                for record in batch:
                    try:
                        gift_item = GiftItem(
                            id=record['id'],
                            title=record['title'],
                            price=float(record['price']),
                            image_url=record.get('image_url', ''),
                            merchant=record['merchant'],
                            url=record['url'],
                            affiliate_url=record.get('affiliate_url', ''),
                            occasion=record.get('occasion', ''),
                            occasions=record.get('occasions', []),
                            review_count=record.get('review_count', 0),
                            review_average=record.get('review_average', 0.0),
                            updated_at=int(datetime.now().timestamp())
                        )
                        gift_items.append(gift_item)
                    except Exception as e:
                        logger.warning(f"ãƒ¬ã‚³ãƒ¼ãƒ‰å¤‰æ›ã‚¨ãƒ©ãƒ¼ ({record.get('id', 'unknown')}): {e}")
                        continue
                
                # Meilisearchã«ä¸€æ‹¬æŒ¿å…¥
                success = await self.meilisearch_service.add_products(gift_items)
                if success:
                    total_updated += len(gift_items)
                    logger.info(f"Meilisearchæ›´æ–°: {total_updated}/{len(records)}ä»¶å®Œäº†")
                else:
                    logger.error(f"Meilisearchæ›´æ–°å¤±æ•—: batch {i//batch_size + 1}")
                    return False
            
            logger.info(f"Meilisearchæ›´æ–°å®Œäº†: {total_updated}ä»¶")
            return True
            
        except Exception as e:
            logger.error(f"Meilisearchæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def update_vector_store(self, records: List[Dict]) -> bool:
        """
        ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®æ›´æ–°ï¼ˆå·®åˆ†å‡¦ç†ï¼‰
        
        Args:
            records: æ›´æ–°ã™ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰
            
        Returns:
            æ›´æ–°æˆåŠŸãƒ•ãƒ©ã‚°
        """
        if not self.vector_store or not self.embeddings:
            logger.warning("ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã¾ãŸã¯åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
        
        try:
            # æ–°è¦ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
            texts = []
            metadatas = []
            
            for record in records:
                # å•†å“æƒ…å ±ã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–
                text = f"{record['title']} {record.get('merchant', '')} {record.get('occasion', '')}"
                metadata = {
                    'id': record['id'],
                    'title': record['title'],
                    'price': record['price'],
                    'merchant': record['merchant'],
                    'occasion': record.get('occasion', ''),
                    'source': 'rakuten_uchiwai'
                }
                
                texts.append(text)
                metadatas.append(metadata)
            
            # ãƒãƒƒãƒå‡¦ç†ã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒ»è¿½åŠ 
            batch_size = 100  # åŸ‹ã‚è¾¼ã¿APIã®åˆ¶é™è€ƒæ…®
            total_added = 0
            
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                batch_metadatas = metadatas[i:i + batch_size]
                
                # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«è¿½åŠ 
                self.vector_store.add_texts(
                    texts=batch_texts,
                    metadatas=batch_metadatas
                )
                
                total_added += len(batch_texts)
                logger.info(f"ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢æ›´æ–°: {total_added}/{len(texts)}ä»¶å®Œäº†")
            
            logger.info(f"ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢æ›´æ–°å®Œäº†: {total_added}ä»¶")
            return True
            
        except Exception as e:
            logger.error(f"ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def run_daily_update(self) -> Dict[str, Any]:
        """
        æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿æ›´æ–°ã‚¸ãƒ§ãƒ–å®Ÿè¡Œ
        
        Returns:
            å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼
        """
        start_time = datetime.now()
        summary = {
            'start_time': start_time.isoformat(),
            'success': False,
            'total_records': 0,
            'valid_records': 0,
            'invalid_records': 0,
            'meilisearch_updated': False,
            'vector_store_updated': False,
            'errors': []
        }
        
        try:
            logger.info("ğŸ”„ æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿æ›´æ–°é–‹å§‹")
            
            # Step 1: æœ€æ–°ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
            latest_file = self.find_latest_data_file()
            if not latest_file:
                summary['errors'].append("æœ€æ–°ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return summary
            
            # Step 2: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»æ¤œè¨¼
            valid_records, invalid_records = self.load_and_validate_data(latest_file)
            summary['total_records'] = len(valid_records) + len(invalid_records)
            summary['valid_records'] = len(valid_records)
            summary['invalid_records'] = len(invalid_records)
            
            if not valid_records:
                summary['errors'].append("æœ‰åŠ¹ãªãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
                return summary
            
            # Step 3: Meilisearchæ›´æ–°
            if self.meilisearch_service:
                meilisearch_success = await self.update_meilisearch(valid_records)
                summary['meilisearch_updated'] = meilisearch_success
                if not meilisearch_success:
                    summary['errors'].append("Meilisearchæ›´æ–°å¤±æ•—")
            
            # Step 4: ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢æ›´æ–°
            if self.vector_store:
                vector_success = self.update_vector_store(valid_records)
                summary['vector_store_updated'] = vector_success
                if not vector_success:
                    summary['errors'].append("ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢æ›´æ–°å¤±æ•—")
            
            # Step 5: æˆåŠŸåˆ¤å®š
            summary['success'] = (
                summary['meilisearch_updated'] or summary['vector_store_updated']
            ) and len(summary['errors']) == 0
            
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            
            logger.info(f"âœ… æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿æ›´æ–°å®Œäº†: {processing_time:.1f}ç§’")
            logger.info(f"   æœ‰åŠ¹ãƒ¬ã‚³ãƒ¼ãƒ‰: {summary['valid_records']}ä»¶")
            logger.info(f"   ç„¡åŠ¹ãƒ¬ã‚³ãƒ¼ãƒ‰: {summary['invalid_records']}ä»¶")
            
            return summary
            
        except Exception as e:
            logger.error(f"æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            summary['errors'].append(str(e))
            return summary


# ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³å®Ÿè¡Œç”¨
async def main():
    """æ—¥æ¬¡æ›´æ–°ã‚¸ãƒ§ãƒ–ã®ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³å®Ÿè¡Œ"""
    updater = DataUpdater()
    result = await updater.run_daily_update()
    
    print(f"æ›´æ–°çµæœ: {result}")
    
    if not result['success']:
        exit(1)  # ã‚¨ãƒ©ãƒ¼çµ‚äº†


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())